{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SwatGrains 2020 Data Analysis: BondInfo.txt file - Contact Networks and Force Distribution\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1. Load Data\n",
    "\n",
    "Loads a set of bondInfo.txt files as hierarchically indexed DataFrames, which contain each jammed trial indexed by trial name and original row number. DataFrames are stored in the dictionary `dataDict`, and are indexed by abbreviated file names for easier iteration.\n",
    "In order to identify which trials are jammed, a Pressures.txt file has to be generated for each data file via Info_Analysis.ipynb.\n",
    "\n",
    "This cell also defines the `sort_particles()` function, which takes bond data and translates it into a DataFrame of particles identified by location and particle type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sp\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def get_pressures(file):\n",
    "    # the Pressures.txt file (created in Info_Analysis.ipynb) contains the seeds of all jammed trials, which we use to filter our data.\n",
    "    # for some of our analysis, we'll also need pressures corresponding to each trial, which this file gives us.\n",
    "    filePath=file.split('/')\n",
    "    filePath[-1]=filePath[-1].replace('BondInfo','Pressures')\n",
    "    trialPressures=pd.read_csv('/'.join(filePath), sep=' ', index_col=0)['pressure']\n",
    "    if file[-8]=='6':\n",
    "        trialPressures=trialPressures/4\n",
    "    return trialPressures\n",
    "\n",
    "def read_data(filename):\n",
    "    dataset=pd.read_csv(filename, usecols=range(7), sep=' ', header=0, names=['x1','y1','x2','y2','energy','force','bondType']) # read bondInfo.txt\n",
    "    dataset['trialName']=dataset.groupby((dataset.x1=='>>>>>').cumsum())['y1'].transform(lambda x: x.max()) # group by trial dividers, recognized by >>>>>, and label each row with group name\n",
    "    dataset = dataset[dataset.x1 != '>>>>>'].reset_index(drop=True) # remove trial dividers\n",
    "\n",
    "    # Generate higher level index\n",
    "    dataset.index.rename('rowNumber', inplace=True)\n",
    "    dataset.set_index('trialName',append=True,inplace=True)\n",
    "    dataset.index=dataset.index.swaplevel()\n",
    "    dataset=dataset.astype('float32') # convert all columns to float32 - 7 decimal places of precision is plenty\n",
    "    dataset['bondType']=dataset['bondType'].astype('int16') # except bond type, which is a column of integers\n",
    "    dataset['normalizedForce']=dataset.groupby('trialName')['force'].transform(lambda x: x/x.mean())\n",
    "    dataset[['x1','y1','x2','y2','normalizedForce']]=(1e6*dataset[['x1','y1','x2','y2','normalizedForce']]).astype('int')\n",
    "    return dataset.loc[get_pressures(filename).index]\n",
    "\n",
    "\n",
    "def sort_particles(dataset, types=[0,1,2]):\n",
    "    # same function as is used in cell 2.1 - sort particles into types via bondType column.\n",
    "    # returns a dataframe of every coordinate in bondInfo - duplicates included.\n",
    "    small_mobile=pd.DataFrame()\n",
    "    large_mobile=pd.DataFrame()\n",
    "    pins=pd.DataFrame()\n",
    "\n",
    "    for bondGroup in dataset.groupby('bondType'):\n",
    "        coords1=bondGroup[1][['x1','y1']].rename(columns={'x1':'x','y1':'y'})\n",
    "        coords2=bondGroup[1][['x2','y2']].rename(columns={'x2':'x','y2':'y'})\n",
    "        # case-by-case for each bond type\n",
    "        if bondGroup[0]==0:\n",
    "            # small-small\n",
    "            small_mobile=pd.concat([small_mobile,coords1,coords2])\n",
    "\n",
    "        if bondGroup[0]==1:\n",
    "            # small-large\n",
    "            small_mobile=pd.concat([small_mobile,coords1])\n",
    "            large_mobile=pd.concat([large_mobile,coords2])\n",
    "\n",
    "        if bondGroup[0]==2:\n",
    "            # large-large\n",
    "            large_mobile=pd.concat([large_mobile,coords1,coords2])\n",
    "\n",
    "        if bondGroup[0]==3:\n",
    "            # pin-small\n",
    "            small_mobile=pd.concat([small_mobile,coords2])\n",
    "            pins=pd.concat([pins,coords1])\n",
    "\n",
    "        if bondGroup[0]==4:\n",
    "            # pin-large\n",
    "            large_mobile=pd.concat([large_mobile,coords2])\n",
    "            pins=pd.concat([pins,coords1])\n",
    "    # Cut off coordinate values at six decimals - this accounts for issues with floating point arithmetic\n",
    "    large_mobile=large_mobile.round(6)\n",
    "    small_mobile=small_mobile.round(6)\n",
    "    pins=pins.round(6)\n",
    "\n",
    "    sortedFrame=pd.concat([large_mobile.assign(particleType=2),small_mobile.assign(particleType=1), pins.assign(particleType=0)])\n",
    "    return  sortedFrame[sortedFrame.particleType.isin(types)] # return combined dataframes, with additional column for type\n",
    "\n",
    "filenames={'non0_07-11':'simulation_data/non_BondInfo07-11.txt', 'non0_07-13':'simulation_data/non_BondInfo07-13.txt',\n",
    "           'non0_08-02':'simulation_data/ZeroPinData/non_BondInfo08-02.txt', 'non0_06-20':'simulation_data/ZeroPinData/non_BondInfo06-20.txt',\n",
    "           'squ16_08-05':'simulation_data/squ_BondInfoNp16_08-05.txt', 'squ64_08-05':'simulation_data/squ_BondInfoNp64_08-05.txt','squ36_07-11':'simulation_data/squ_BondInfo07-11.txt',\n",
    "           'squ64_07-12':'simulation_data/squ_BondInfo07-12.txt', 'squ64_07-13':'simulation_data/squ_BondInfo07-13.txt',\n",
    "           'tri36_07-11':'simulation_data/tri_BondInfo07-11.txt', 'tri64_08-08':'simulation_data/tri_BondInfoNp64_08-08.txt', 'tri64_07-12':'simulation_data/tri_BondInfo07-12.txt',\n",
    "           'ran36_07-11':'simulation_data/ran_BondInfo07-11.txt', 'squ100_07-19':'simulation_data/squ_BondInfo07-19.txt', 'tri100_07-19':'simulation_data/tri_BondInfo07-19.txt', 'ran100_07-19':'simulation_data/ran_BondInfo07-19.txt'}\n",
    "\n",
    "files = ['non0_07-11','squ36_07-11', 'squ64_07-12', 'squ100_07-19','ran36_07-11','ran100_07-19', 'tri100_07-19']\n",
    "dataDict={file:read_data(filenames[file]) for file in tqdm_notebook(files)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict['non0_07-11'].index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analysis on Entire File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Contact numbers\n",
    "Creates the DataFrame `dfContacts`, which identifies each particle with its corresponding number of contacts, indexed by particle type (pin, small, large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contactsDict={file:sort_particles(dataDict[file]).groupby(['x','y','particleType','trialName']).size().reset_index(name='contacts').set_index('particleType') for file in tqdm_notebook(files)} # dataframe of particle coordinates, with contact number and trial labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Extrapolate $z_\\text{iso}$ as pressure goes to 0\n",
    "This cell fits $z(P)$ to a square root function. We expect $\\Delta z\\propto\\sqrt{\\Delta\\phi},$ and by extension\n",
    "$$z(P)=c\\sqrt{P}+z_\\text{iso},$$\n",
    "where $c$ is (we hope) some unit-dependent constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def power_fit(xdata, coeff, intercept):\n",
    "    return coeff*np.sqrt(xdata)+intercept\n",
    "\n",
    "colormap = matplotlib.cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', 'r', '#FFFFFF', '#FFFFFF']) # classic\n",
    "#colormap = matplotlib.cycler('color',['#FAD55C','#68C182','#267DB3','#ED6647','#8561C8','#000000', '#fef9e6', '#c0dff2']) # blue-yellow\n",
    "plt.rc('axes',prop_cycle=colormap)\n",
    "\n",
    "\n",
    "def get_pressures(file):\n",
    "    # for some of our data, we'll need pressures corresponding to each trial, which is found in Pressures files generated by jamming_plots.ipynb.\n",
    "    filePath=file.split('/')\n",
    "    filePath[-1]=filePath[-1].replace('BondInfo','Pressures')\n",
    "    trialPressures=pd.read_csv('/'.join(filePath), sep=' ', index_col=0)['pressure']\n",
    "    if file[-8]=='6':\n",
    "        trialPressures=trialPressures/4\n",
    "    return trialPressures\n",
    "intercepts=[]\n",
    "plt.figure(figsize=(8,8))\n",
    "for colorCounter, file in tqdm_notebook(enumerate(files)):\n",
    "    meanContacts=contactsDict[file].loc[[1,2]].groupby('trialName')['contacts'].mean()\n",
    "    pressureData = pd.concat([get_pressures(filenames[file]),meanContacts], axis=1)\n",
    "    pressureData = pressureData[pressureData.pressure>1e-8]\n",
    "    pressureData.index.name='trialName'\n",
    "    [coeff,intercept], covariance = sp.curve_fit(power_fit, pressureData['pressure'], pressureData['contacts'])\n",
    "    perror=np.sqrt(np.diag(covariance))\n",
    "    plt.plot(pressureData['pressure'], pressureData['contacts'],'C{}.'.format(colorCounter), alpha=0.2)\n",
    "    xValues=np.linspace(5e-4,1.35,10000)\n",
    "    yValues=power_fit(xValues, coeff, intercept)\n",
    "    plt.plot(xValues,yValues, 'C{}'.format(colorCounter), alpha=0.7, label='{name}, $z_c={z:.3f}+/-{err:.3f}$'.format(name=file, z=intercept,err=perror[1]))\n",
    "    intercepts.append(intercept)\n",
    "\n",
    "    # Export delta z values to Pressures.txt file for plotting in jamming_plots.ipynb\n",
    "    #pressurePath=filenames[file].split('/')\n",
    "    #pressurePath[-1]=pressurePath[-1].replace('BondInfo','Pressures')\n",
    "    #pressureData['delta_z']=pressureData['contacts']-intercept\n",
    "    #pressureData.drop('contacts', axis=1, inplace=True)\n",
    "    #pressureData.to_csv('/'.join(pressurePath), sep=' ')\n",
    "\n",
    "    # Save fit function values for aggregate analysis in Cell 1.3\n",
    "    #parameters = pd.DataFrame({'coeff': coeff, 'intercept': intercept, 'err1': perror[0], 'err2': perror[1]}, index=[file])\n",
    "    #print(parameters)\n",
    "    #parameters.to_csv('simulation_data/contact_number_parameters.txt', mode = 'a', sep = ' ', header = False)\n",
    "\n",
    "plt.ylabel('mean contact number', fontsize=14)\n",
    "plt.xlabel('pressure', fontsize=14)\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.title('z vs. pressure for 230 particle square lattices', fontsize=16)#.format(num=int(dfContacts.groupby('trialName').size().mean()),z=intercept, err=perror[1]))\n",
    "plt.savefig('plots/presentation/z_vs_p-squares.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "#n_f=[0,0.0696,0.2783]\n",
    "#plt.plot(n_f,intercepts,'C0.')\n",
    "#plt.ylabel('critical contact number')\n",
    "#plt.xlabel('pin density')\n",
    "#plt.title('$z_c$ vs $n_f$')\n",
    "#plt.savefig('plots/zc_vs_nf-non_corrected-comparison.png', dpi=300)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Standalone: Contact Number Summary\n",
    "Since we're pulling data from the contact_number_parameters.txt file exclusively, this cell contains the same imports as the first cell and the power fit function from the last cell and runs without any prior initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sp\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "def power_fit(xdata, coeff, intercept):\n",
    "    return np.sqrt(coeff*xdata)+intercept\n",
    "\n",
    "\n",
    "contactParameters = pd.read_csv('simulation_data/contact_number_parameters.txt', sep=' ', header=None, names=['trial','coefficient','z_iso','err1','err2'])\n",
    "#xValues=np.linspace(0,1.5,100)\n",
    "#yList=[0,0,0,0,36,64,36,64]\n",
    "#for row in contactParameters.iterrows():\n",
    "#    yList.append((row[1]['num_pins'],power_fit(xValues, row[1]['coefficient'],row[1]['z_iso'])))\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "\n",
    "num_pins_array=np.array([0,0,0,0,36,64,36,64])\n",
    "ax2 = fig.add_subplot(1,3,1)\n",
    "ax2.margins(0.15)\n",
    "plt.xlabel('number of pins')\n",
    "plt.ylabel('$z_{iso}$')\n",
    "plt.title('isostatic number $z_{iso}$ vs. number of pins')\n",
    "\n",
    "ax3 = fig.add_subplot(1,3,2)\n",
    "ax3.margins(0.15)\n",
    "plt.xlabel('number of pins')\n",
    "plt.ylabel('$c$')\n",
    "plt.title('coefficient $c$ vs. number of pins')\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(1,3,3)\n",
    "ax1.margins(0.15)\n",
    "plt.xlabel('number of particles')\n",
    "plt.ylabel('$c$')\n",
    "plt.title('coefficient $c$ vs. number of particles')\n",
    "\n",
    "for row, num_pins, num_particles in zip(contactParameters.iterrows(), num_pins_array, [230,230,230,920,230,230,230,230]):\n",
    "    if row[1]['trial'][:3]=='non':\n",
    "        markerstyle='X'\n",
    "    elif row[1]['trial'][:3]=='squ':\n",
    "        markerstyle='s'\n",
    "    elif row[1]['trial'][:3]=='tri':\n",
    "        markerstyle='^'\n",
    "    ax2.errorbar(num_pins, row[1]['z_iso'], yerr=row[1]['err2'], fmt='.', ms=8, alpha=0.6, marker=markerstyle, label=row[1]['trial'])\n",
    "    ax3.errorbar(num_pins, row[1]['coefficient'], yerr=row[1]['err1'], fmt='.',  ms=8, alpha=0.6, marker=markerstyle, label=row[1]['trial'])\n",
    "    ax1.errorbar(num_particles, row[1]['coefficient'], yerr=row[1]['err1'], fmt='.',  ms=8, alpha=0.6, marker=markerstyle, label=row[1]['trial'])\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('plots/contact_number_trends_new.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sp\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "contactParameters = pd.read_csv('simulation_data/contact_number_parameters.txt', sep=' ', header=None, names=['trial','coefficient','z_iso','err1','err2', 'lambda'])\n",
    "contactParameters['w_pins']=[0,0,0,0,36/266,64/294,36/266,64/294, 16/246, 64/294,64/294, 36/266,100/330,100/330,100/330]\n",
    "plt.figure(figsize=(8,8))\n",
    "contactParameters['geo']=contactParameters['trial'].transform(lambda x: x[:3])\n",
    "#contactParameters.drop(contactParameters[contactParameters.geo=='non'].index, inplace=True)\n",
    "for colorCounter, geometry in enumerate(contactParameters.groupby('geo'),0):\n",
    "    if geometry[0]=='non':\n",
    "        marker='X'\n",
    "    elif geometry[0]=='squ':\n",
    "        marker='s'\n",
    "    elif geometry[0]=='tri':\n",
    "        marker='^'\n",
    "    elif geometry[0]=='ran':\n",
    "        marker='d'\n",
    "    plt.errorbar(geometry[1]['w_pins'], geometry[1]['z_iso'], yerr=geometry[1]['err2'], fmt=marker, alpha=0.6, label=geometry[0], color='C{}'.format(colorCounter))\n",
    "\n",
    "xValues=np.linspace(*plt.xlim(), 1000)\n",
    "yValues=4*(1-xValues)/(1-xValues/2)\n",
    "plt.plot(xValues,yValues, label='theoretical expectation', color='C4')\n",
    "plt.figtext(0.25,0.4,r'$z_c(w)=2d\\displaystyle\\frac{1-w}{1-w/2}$', fontsize=17)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.ylabel('$z_c$', fontsize = 20)\n",
    "plt.xlabel('$w_{pins}$', fontsize = 20)\n",
    "plt.title('critical $z$ vs $w_{pins}$', fontsize = 23)\n",
    "#plt.savefig('plots/presentation/z_c_vs_wpins-comparison.png', dpi=300)\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. (Deprecated). Finding contact number with NetworkX\n",
    "After failing to effectively plot topology with NetworkX, we revisited the module, attempting to use its graph.neighbors() function to find contact numbers without too much fuss. Unfortunately, NetworkX is not well-suited to such large data sets, and this script, while theoretically functional, ends up being incredibly inefficient and has never finished running."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import networkx as nx\n",
    "\n",
    "contactNumbers=[]\n",
    "entireFile['sourceNodes']=entireFile['x1'].round(6).astype('str')+' '+entireFile['y1'].round(6).astype('str')\n",
    "entireFile['targetNodes']=entireFile['x2'].round(6).astype('str')+' '+entireFile['y2'].round(6).astype('str')\n",
    "list_of_trials=entireFile.index.get_level_values('trialName')\n",
    "for trial in list_of_trials:\n",
    "#trialName=entireFile.index.get_level_values('trialName')[0] # get name of first trial\n",
    "#headerNames=['geometry','seed','num','pins','small_radius','something'] # TODO: what is this last value in the header? pin spacing?\n",
    "#trialInfo={}\n",
    "#for key, label in zip(headerNames,trialName.split('-')): # retrieve info from name\n",
    "#    trialInfo[key]=label\n",
    "    bondInfo=entireFile.loc[[trial]]\n",
    "\n",
    "    #print(bondInfo['sourceNodes'])\n",
    "    network=nx.from_pandas_edgelist(bondInfo,'sourceNodes','targetNodes',edge_attr=True)\n",
    "\n",
    "    nonPinEdges=[(node1,node2) for (node1,node2, data) in network.edges(data=True) if data['bondType']<3]\n",
    "    nonPinGraph=nx.Graph(nonPinEdges)\n",
    "    #print(\"average number of contact (no pins):\", 2*nonPinGraph.number_of_edges()/nonPinGraph.number_of_nodes())\n",
    "    #print(\"average number of contact (with pins):)\", 2*network.number_of_edges()/network.number_of_nodes())\n",
    "    meanContacts=2*nonPinGraph.number_of_edges()/nonPinGraph.number_of_nodes()\n",
    "    for node in nonPinGraph.nodes:\n",
    "        contactNumbers.append(len(list(nonPinGraph.neighbors(node))))\n",
    "\n",
    "plt.hist(contactNumbers, bins=np.linspace(min(contactNumbers)-0.25,max(contactNumbers)+0.25, (max(contactNumbers)-min(contactNumbers)+1)*2),color='C0')\n",
    "plt.title('Contact number barplot for {} lattice of {} pins, mean = {}'.format(trialInfo['geometry'],trialInfo['pins'],meanContacts))\n",
    "plt.xlabel('Contact Number')\n",
    "#plt.savefig('plots/contact_barplot_{}{}.png'.format(*[trialInfo[x] for x in ['geometry','pins']]), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining $N_{excess}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib\n",
    "#matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "from scipy import stats\n",
    "# What's really important: picking a fun color scheme\n",
    "#colormap = matplotlib.cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', 'r', '#FFFFFF', '#FFFFFF']) # classic\n",
    "#colormap = matplotlib.cycler('color',['#FAD55C','#68C182','#267DB3','#ED6647','#8561C8','#000000', '#fef9e6', '#c0dff2']) # blue-yellow\n",
    "#plt.rc('axes',prop_cycle=colormap)\n",
    "\n",
    "fig, (axRan, axSqu, axTri) = plt.subplots(3,1,figsize=(8,10))\n",
    "for colorCounter, file in enumerate(tqdm_notebook(files)):\n",
    "    N_pp=dataDict[file][dataDict[file].bondType<=2].groupby(level=0).size()\n",
    "    N_pf=dataDict[file][dataDict[file].bondType>=3].groupby(level=0).size()\n",
    "    num_particles=sort_particles(dataDict[file], types=[1,2]).drop_duplicates(keep='first').groupby('trialName').size()\n",
    "    N_excess=N_pp+N_pf-2*num_particles-1\n",
    "    N_pf=N_pf[N_excess>0]\n",
    "    N_excess=N_excess[N_excess>0]\n",
    "    regression=stats.linregress(N_pf,N_excess)\n",
    "    yValues=np.array([0,50])\n",
    "    xValues=(yValues-regression[1])/regression[0]\n",
    "    if file[:3]=='ran':\n",
    "        axRan.plot(N_pf, N_excess, 's', ms=2, label='{}, slope={slope:.2f}, r={r:.2f}'.format(file, slope=regression[0], r=regression[2]), alpha=0.6, color='C{}'.format(colorCounter))\n",
    "        axRan.plot(xValues,yValues, color='C{}'.format(colorCounter))\n",
    "    elif file[:3]=='squ':\n",
    "        axSqu.plot(N_pf, N_excess, 's', ms=2, label='{}, slope={slope:.2f}, r={r:.2f}'.format(file, slope=regression[0], r=regression[2]), alpha=0.6, color='C{}'.format(colorCounter))\n",
    "        axSqu.plot(xValues,yValues, color='C{}'.format(colorCounter))\n",
    "    elif file[:3]=='tri':\n",
    "        axTri.plot(N_pf, N_excess, 's', ms=2, label='{}, slope={slope:.2f}, r={r:.2f}'.format(file, slope=regression[0], r=regression[2]), alpha=0.6, color='C{}'.format(colorCounter))\n",
    "        axTri.plot(xValues,yValues, color='C{}'.format(colorCounter))\n",
    "\n",
    "for axis in [axRan,axSqu,axTri]:\n",
    "    axis.set(aspect='equal',adjustable='datalim')\n",
    "    axis.set_xlim(0,95)\n",
    "    axis.set_ylim(0,50)\n",
    "    axis.legend(loc='upper left')\n",
    "\n",
    "plt.xlabel('$N_{pf}$')\n",
    "axSqu.set_ylabel('$N_{excess}$')\n",
    "fig.suptitle('$N_{excess}$ vs particle pin contacts', y=0.93)\n",
    "#plt.savefig('plots/Nexcess_vs_Npf-July20-regressed.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Finding Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Find $n_6$ of Large Neighbors\n",
    "Iterates through every trial identifying the number of large neighbors in n_6 for each large particle\n",
    "Returns a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def get_large_neighbors(particles):\n",
    "    # Uses scipy.spatial.KDTree to identify nearest six neighbors, then returns array of number of large neighbors for each large particle\n",
    "\n",
    "    # Initialize data\n",
    "    particles=particles.drop_duplicates(keep='first') # just want one instance of each particle, rather than once per contact\n",
    "    particles=particles[particles.particleType>0] # remove pins\n",
    "    tree=spatial.KDTree(list(zip(particles['x'],particles['y']))) # load coordinates as KDTree\n",
    "    large_particles=particles[particles.particleType==2] # series of large particles, which we iterate over\n",
    "    large_neighbors=np.array([],dtype='int16') # array to add number of neighbors for each particle to (within the trial)\n",
    "\n",
    "    # Find neighbors of each particle\n",
    "    try:\n",
    "        for neighbor_index in tree.query(list(zip(large_particles['x'],large_particles['y'])),k=7)[1]: # Get indices of six closest particle neighbors (seven including self)\n",
    "            try:\n",
    "                neighborTypes=particles.iloc[neighbor_index].groupby('particleType').size() # count how many neighbors are large\n",
    "                large_neighbors=np.append(large_neighbors,neighborTypes[2]) # add large neighbor count to array\n",
    "\n",
    "            except IndexError: # Error case where a data set has fewer than six particles\n",
    "                raise IndexError\n",
    "\n",
    "        return large_neighbors\n",
    "\n",
    "    except ValueError: # Error case where a data set has no large particles\n",
    "        raise ValueError\n",
    "\n",
    "def write_neighbors(neighbor_column):\n",
    "    # Writes series of large neighbors as a new column in large_neighbors.txt for future reference\n",
    "\n",
    "    # Load text file\n",
    "    try:\n",
    "        data = pd.read_csv('simulation_data/new_large_neighbors.txt', sep=' ', dtype='Int16')\n",
    "    except:\n",
    "        print('Note: simulation_data/new_large_neighbors.txt was empty or missing.')\n",
    "        data=pd.DataFrame(dtype='Int16')\n",
    "\n",
    "    data=pd.concat([data,neighbor_column], axis=1) # add new column to existing data\n",
    "    data=data.loc[:,~data.columns.duplicated(keep='last')] # in case of duplicates, overwrite\n",
    "    data.to_csv('simulation_data/new_large_neighbors.txt', sep = ' ', index=False) # write new data back to text file\n",
    "    return data\n",
    "\n",
    "def write_n_6(n6_column, file):\n",
    "    # Writes series of n_6 values as a new column in corresponding Pressures file\n",
    "\n",
    "    # Load text file\n",
    "    filePath=file.split('/')\n",
    "    filePath[-1]=filePath[-1].replace('BondInfo','Pressures')\n",
    "    try:\n",
    "        data = pd.read_csv('/'.join(filePath), sep=' ', index_col=0)\n",
    "    except:\n",
    "        print('Note: {} was empty or missing.'.format('/'.join(filePath)))\n",
    "        data=pd.DataFrame()\n",
    "\n",
    "    data['n_6']=n6_column # add new column to dataframe - this method of creating a column skips any values in n6_column whose trial names aren't in Pressures.txt (not actually jamming)\n",
    "    data.index.name='trialName'\n",
    "    data.to_csv('/'.join(filePath), sep = ' ') # write new data back to text file\n",
    "    return data\n",
    "\n",
    "for file in files:\n",
    "    print('Processing:', file)\n",
    "    # Find number of large particle neighbors for all trials in BondInfo (will take one or two minutes)\n",
    "    large_neighbors=np.array([], dtype=np.int16) # initialize array for number of neighbors for entire BondInfo file\n",
    "    n_6=[]\n",
    "    for sorted_trial in tqdm_notebook(sort_particles(dataDict[file]).groupby('trialName')):\n",
    "        try:\n",
    "            trial_neighbors=get_large_neighbors(sorted_trial[1])\n",
    "            large_neighbors=np.append(large_neighbors, trial_neighbors-1) # add array of large neighbor values for this trial to the overall array\n",
    "            n_6.append([sorted_trial[0],trial_neighbors[trial_neighbors==7].size/trial_neighbors.size]) # get n_6, percentage of large particles with six large neighbors\n",
    "        except:\n",
    "            print('EXCEPTION ON TRIAL', sorted_trial[0]) # this warns us if we can't get large neighbors for a trial, and skips over it\n",
    "\n",
    "    # Export neighbor data\n",
    "    n_6=np.asarray(n_6) # list to array\n",
    "    n_6=pd.Series(n_6[:,1],index=n_6[:,0]) # array to series\n",
    "    write_neighbors(pd.Series(large_neighbors, name=file, dtype='Int16'))\n",
    "    write_n_6(n_6, filenames[file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Creates a neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as sp\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "def read_neighbors(filename):\n",
    "    data = pd.read_csv(filename, sep=' ').astype('Int16')\n",
    "    return data\n",
    "\n",
    "neighbors = read_neighbors('simulation_data/new_large_neighbors.txt')\n",
    "neighbors = neighbors.reindex(sorted(neighbors.columns), axis=1)\n",
    "plt.figure(figsize=(10,6))\n",
    "for label, content in neighbors.items():\n",
    "    sns.distplot(content.dropna(), hist=True, kde=True, bins=np.linspace(-0.5,6.5,8), hist_kws={'histtype':'step','linewidth':1,'alpha':0.8}, kde_kws={'linewidth': 0},\n",
    "                 label='{name}, $n_6={n:.4f}$'.format(name=label, n=content[content==6].size/content.dropna().size))\n",
    "\n",
    "plt.ylim(0,0.35)\n",
    "plt.ylabel('Probability Density')\n",
    "plt.xlabel('Number of Large Neighbors')\n",
    "plt.title('Normalized Distribution of Closest Six Neighbors of Large Particles')\n",
    "plt.legend()\n",
    "#plt.savefig('plots/n_6_histogram-comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborDensities=pd.DataFrame(dtype='float32')\n",
    "for trial in neighbors:\n",
    "    neighborDensities[trial]=neighbors.groupby(trial).size()/neighbors[trial].dropna().size\n",
    "\n",
    "neighborDensities['mean']=neighborDensities.mean(axis=1)\n",
    "neighborDensities=neighborDensities.transform(lambda x: x-x['non0'], axis=1)\n",
    "neighborDensities['left']=neighborDensities.index-0.5\n",
    "neighborDensities['right']=neighborDensities.index+0.5\n",
    "xValues=pd.concat([neighborDensities['left'],neighborDensities['right']]).sort_index()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for trial in neighbors:\n",
    "    plt.plot(xValues,neighborDensities[trial].repeat(2), label=trial, alpha=0.8)\n",
    "\n",
    "plt.title('Closest Six Neighbors of Large Particles, normalized and zeroed to non0')\n",
    "plt.xlabel('Number of Large Neighbors')\n",
    "plt.legend()\n",
    "#plt.savefig('plots/n_6_zeroed-comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGrid=[('squ_PressuresNp16_08-05.txt', 'squ16'), ('squ_PressuresNp64_08-05.txt', 'squ64'), ('tri_PressuresNp64_08-08.txt', 'tri64'), ('ZeroPinData/non_Pressures08-02.txt', 'non0')]\n",
    "fig=plt.figure(figsize=(8,8))\n",
    "for counter, (file, name) in enumerate(plotGrid, 1):\n",
    "    fig.add_subplot(2,2,counter)\n",
    "    n6_data=pd.read_csv('simulation_data/'+file, sep=' ', index_col=0)\n",
    "    plt.plot(n6_data['pressure'], n6_data['n_6'], 'C{}.'.format(counter-1), alpha=0.3)\n",
    "    plt.xlim(-0.1,2.2)\n",
    "    plt.ylim(-0.01,0.15)\n",
    "    plt.title(name)\n",
    "#plt.legend()\n",
    "fig.suptitle('n_6 vs. pressure', fontsize=14)\n",
    "fig.text(0.5, 0.06, 'pressure', ha='center')\n",
    "fig.text(0.04, 0.5, '$n_6$', va='center', rotation='vertical')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "#plt.savefig('plots/n_6_vs_pressure-comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Force distribution\n",
    "Plots histograms of the normalized contact force, first for the entire data set, then broken up by bond type, then normalized as a kernel density estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = matplotlib.cycler('color',['#FAD55C','#68C182','#267DB3','#ED6647','#8561C8','#000000', '#fef9e6', '#c0dff2']) # blue-yellow\n",
    "plt.rc('axes',prop_cycle=colormap)\n",
    "\n",
    "fig2 = plt.figure(figsize=(12,18))\n",
    "hist1=fig2.add_subplot(3,1,1)\n",
    "sns.distplot(entireFile['normalizedForce']/1e6, hist=True, kde=False,\n",
    "             bins=1000, color = 'darkblue',\n",
    "             hist_kws={'histtype':'step','linewidth':1,'alpha':0.95},\n",
    "             )\n",
    "hist1.set_xlim(-0.15,5.5)\n",
    "hist1.set_title('force distribution for square lattice with $n_f={0.157}$, 07-11 2020')\n",
    "hist1.set_xlabel('')\n",
    "hist1.set_ylabel('counts')\n",
    "hist1.set_yscale('log')\n",
    "\n",
    "hist2=fig2.add_subplot(3,1,2)\n",
    "entireFile.groupby('bondType')['normalizedForce'].apply(lambda x: sns.distplot(x/1e6, hist=True, kde=False, bins=1000, hist_kws={'histtype':'step','linewidth':1,'alpha':0.95}, label='bondType {}'.format(x.name)))\n",
    "hist2.set_title('distribution by bond type')\n",
    "hist2.set_xlabel('')\n",
    "hist2.set_ylabel('counts')\n",
    "hist2.set_yscale('log')\n",
    "hist2.set_xlim(-0.15,5.5)\n",
    "hist2.legend()\n",
    "\n",
    "hist3=fig2.add_subplot(3,1,3)\n",
    "entireFile.groupby('bondType')['normalizedForce'].apply(lambda x:sns.kdeplot(x/1e6,label='bondType {}'.format(x.name)))\n",
    "hist3.set_title('probability density KDEs by bond type (linear axes)')\n",
    "hist3.set_xlabel('normalized contact forces')\n",
    "hist3.set_ylabel('probability density')\n",
    "hist3.set_xlim(-0.15,5.5)\n",
    "#hist3.set_yscale('log')\n",
    "hist3.legend()\n",
    "#plt.savefig('plots/07-11-squ36-force_distribution_histogram.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "plt.figure(figsize=(10,6))\n",
    "for file, label in zip(['squ100_07-19','squ36_07-11','non0_07-11'],['100 Pins', '36 Pins', '0 Pins']):\n",
    "    bw = 2*stats.iqr(dataDict[file][dataDict[file].bondType<=2]['normalizedForce']/1e6, rng=(25,75), scale=1.0, nan_policy='omit')/(len(dataDict[file])**(1/3))\n",
    "    plt.hist(dataDict[file][dataDict[file].bondType<=2]['normalizedForce']/1e6, bins=int((dataDict[file][dataDict[file].bondType<=2]['normalizedForce'].max()/1e6-dataDict[file][dataDict[file].bondType>=3]['normalizedForce'].min()/1e6)/bw), density=True, alpha=0.4, label=label)\n",
    "#plt.yscale('log')\n",
    "plt.xlim(0,2)\n",
    "plt.legend()\n",
    "plt.xlabel('normalized force')\n",
    "plt.ylabel('probability density')\n",
    "plt.title('Contact Force Distributions, Peak Behavior')\n",
    "#plt.savefig('plots/presentation/forceDistribution-peak.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict['non0_07-11'][dataDict['non0_07-11'].isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def linear_fit(x, beta, intercept):\n",
    "    return beta*x+intercept\n",
    "\n",
    "bw=0.08\n",
    "fig=plt.figure(figsize=(20,16))\n",
    "locations=[1,2,4,5,7,3,6,9]\n",
    "for location, file in zip(locations,files):\n",
    "    fig.add_subplot(3,3,location)\n",
    "    n, bins = plt.hist(dataDict[file].loc[dataDict[file].bondType<=2,'normalizedForce']/1e6, bins=int(1e-6*dataDict[file].loc[dataDict[file].bondType<=2,'normalizedForce'].max()/bw), histtype='step',alpha=0.6, density=True, label='particle-particle')[:2]\n",
    "    bins=bins[:-1]+np.diff(bins)/2\n",
    "    particleparticle=pd.DataFrame({'normalizedForce':bins,'density':n})\n",
    "    particleparticle=particleparticle[(particleparticle.normalizedForce>3)&(particleparticle.density>0)]#&(particleparticle.normalizedForce<5)]\n",
    "    particleparticle['density']=np.log(particleparticle['density'])\n",
    "    particleregress = stats.linregress(particleparticle['normalizedForce'], particleparticle['density'])\n",
    "    RMS_Particle=np.sqrt(sum((particleparticle['density']-np.exp(linear_fit(particleparticle['normalizedForce'],particleregress[0],particleregress[1])))**2)/len(particleparticle))\n",
    "    if file[:4] != 'non0':\n",
    "        n, bins = plt.hist(dataDict[file].loc[dataDict[file].bondType>=3,'normalizedForce']/1e6, bins=int(1e-6*dataDict[file].loc[dataDict[file].bondType>=3,'normalizedForce'].max()/bw), histtype='step',alpha=0.6, density=True, label='particle-pin')[:2]\n",
    "        bins=bins[:-1]+np.diff(bins)/2\n",
    "        particlepin=pd.DataFrame({'normalizedForce':bins,'density':n})\n",
    "        particlepin=particlepin[(particlepin.normalizedForce>3)&(particlepin.density>0)]#&(particlepin.normalizedForce<5)]\n",
    "        particlepin['density']=np.log(particlepin['density'])\n",
    "        pinregress = stats.linregress(particlepin['normalizedForce'], particlepin['density'])\n",
    "        RMS_Pin=np.sqrt(sum((particlepin['density']-np.exp(linear_fit(particlepin['normalizedForce'],pinregress[0],pinregress[1])))**2)/len(particlepin))\n",
    "    if file[:3]=='ran':\n",
    "        plt.xlim(-20,210)\n",
    "#    if file[:3]=='squ':\n",
    "#        plt.xlim(-5,40)\n",
    "#    if file[:3]=='tri':\n",
    "#        plt.xlim(-5,49)\n",
    "    #plt.xlim(-0.4,15)\n",
    "    plt.ylim(3e-5,3)\n",
    "    xValues=np.array(plt.xlim())\n",
    "    plt.plot(xValues, np.exp(linear_fit(xValues, particleregress[0], particleregress[1])), 'C0', label='slope = {slope:.4f}, r={r:.4f}\\n $\\chi^2$={rms:.4f}, stderr={s:.3f}'.format(slope=particleregress[0],rms=RMS_Particle, s = particleregress[4], r=particleregress[2]))\n",
    "    if file[:4]!='non0':\n",
    "        plt.plot(xValues, np.exp(linear_fit(xValues, pinregress[0],pinregress[1])), 'C1', label='slope = {slope:.4f}, r={r:.4f}\\n $\\chi^2$={rms:.3f}, stderr={s:.3f}'.format(slope=pinregress[0],rms=RMS_Pin, s=pinregress[4], r=pinregress[2]))\n",
    "    plt.title(file)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('normalized force')\n",
    "    plt.ylabel('probability density')\n",
    "    plt.legend()\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "fig.suptitle('Force Distribution for Particles vs. Pins, 230 Particles, Regressed for 3<normalizedForce, July 2020 Data', x=0.5,y=0.95, fontsize=16) # title of entire figure\n",
    "#plt.savefig('plots/force_distribution_particle_pin-comparison-regressed7_23.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw=0.08\n",
    "fig, ((axParticle36,axPins36),(axParticle64,axPins64)) = plt.subplots(2,2, figsize=(15,12))\n",
    "for file in ['squ36_07-11','tri36_07-11','ran36_07-11','non07-11']:\n",
    "    sns.distplot(dataDict[file].loc[dataDict[file].bondType<=2,'normalizedForce']/1e6, bins=int(dataDict[file].loc[dataDict[file].bondType<=2,'normalizedForce'].max()/bw), hist=True, kde=True,\n",
    "                 hist_kws={'histtype':'step','linewidth':1,'alpha':0.85}, kde_kws={'linewidth':0}, ax=axParticle36, label=file)\n",
    "    if file != 'non07-11':\n",
    "        sns.distplot(dataDict[file].loc[dataDict[file].bondType>=3,'normalizedForce']/1e6, bins=int(dataDict[file].loc[dataDict[file].bondType>=3,'normalizedForce'].max()/bw), hist=True, kde=True,\n",
    "                     hist_kws={'histtype':'step','linewidth':1,'alpha':0.85}, kde_kws={'linewidth':0}, ax=axPins36, label=file)\n",
    "for file in ['squ64_07-12','tri64_07-12','non07-11']:\n",
    "    if file != 'non07-11':\n",
    "        sns.distplot(dataDict[file].loc[dataDict[file].bondType<=2,'normalizedForce']/1e6, bins=int(dataDict[file].loc[dataDict[file].bondType<=2,'normalizedForce'].max()/bw), hist=True, kde=True,\n",
    "                     hist_kws={'histtype':'step','linewidth':1,'alpha':0.85}, kde_kws={'linewidth':0}, ax=axParticle64, label=file)\n",
    "        sns.distplot(dataDict[file].loc[dataDict[file].bondType>=3,'normalizedForce']/1e6, bins=int(dataDict[file].loc[dataDict[file].bondType>=3,'normalizedForce'].max()/bw), hist=True, kde=True,\n",
    "                     hist_kws={'histtype':'step','linewidth':1,'alpha':0.85}, kde_kws={'linewidth':0}, ax=axPins64, label=file)\n",
    "    else:\n",
    "        sns.distplot(dataDict[file].loc[dataDict[file].bondType<=2,'normalizedForce']/1e6, bins=int(dataDict[file].loc[dataDict[file].bondType<=2,'normalizedForce'].max()/bw), color='C3', hist=True, kde=True,\n",
    "                     hist_kws={'histtype':'step','linewidth':1,'alpha':0.85}, kde_kws={'linewidth':0}, ax=axParticle64, label=file)\n",
    "axParticle36.legend()\n",
    "axParticle36.set_xlim(-0.2,18)\n",
    "axParticle36.set_ylim(3e-5,1)\n",
    "axParticle36.set_yscale('log')\n",
    "\n",
    "axPins36.legend()\n",
    "axPins36.set_xlim(-0.2,18)\n",
    "axPins36.set_ylim(3e-5,1)\n",
    "axPins36.set_yscale('log')\n",
    "\n",
    "axParticle64.legend()\n",
    "axParticle64.set_xlim(-0.2,18)\n",
    "axParticle64.set_ylim(3e-5,1)\n",
    "axParticle64.set_yscale('log')\n",
    "\n",
    "axPins64.legend()\n",
    "axPins64.set_xlim(-0.2,18)\n",
    "axPins64.set_ylim(3e-5,1)\n",
    "axPins64.set_yscale('log')\n",
    "\n",
    "axParticle36.set_title('particle-particle contacts')\n",
    "axPins36.set_title('particle-pin contacts')\n",
    "axParticle36.set_xlabel('')\n",
    "axParticle64.set_xlabel('')\n",
    "axPins36.set_xlabel('')\n",
    "axPins64.set_xlabel('')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(left=0.07,bottom=0.05,top=0.94, right=0.978)\n",
    "fig.suptitle('particle-particle and particle-pin force distributions, July 2020 data', fontsize=14)\n",
    "fig.text(0.5, 0.01, 'normalized force', ha='center')\n",
    "fig.text(0.02, 0.5, 'probability density', va='center', rotation='vertical')\n",
    "fig.text(0.98, 0.73, '36 pins', va='center', rotation=-90, fontsize=12)\n",
    "fig.text(0.98, 0.27, ' 64 pins', va='center', rotation=-90, fontsize=12)\n",
    "#plt.savefig('plots/force_distribution_bondType_numPins-comparison-07_2020.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Weak Particle-Pin Contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weakPins=dataDict['squ100_07-19'][(dataDict['squ100_07-19']['normalizedForce']<5000)&(dataDict['squ100_07-19']['bondType']>=3)].copy()\n",
    "weakPins['contactsParticle']=weakPins.apply(lambda x: contactsDict['squ100_07-19'].loc[x.name[0],x['x2'],x['y2']]['contacts'], axis=1)\n",
    "weakPins['contactsPin']=weakPins.apply(lambda x: contactsDict['squ100_07-19'].loc[x.name[0],x['x1'],x['y1']]['contacts'], axis=1)\n",
    "weakPins[weakPins.contactsParticle==7].sort_values('normalizedForce').head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topography('squ36_07-11', 100, savefig='plots/presentation/squ36_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "for file in files:\n",
    "    plt.hist(dataDict[file][dataDict[file].bondType>=3].groupby(level=0)['normalizedForce'].nsmallest(2)*1e-6, bins=10**np.linspace(-6,0,13), histtype='step', label=file)\n",
    "plt.axvline(5e-3,alpha=0.5)\n",
    "plt.title('normalized force of two weakest particle-pin bonds per seed')\n",
    "plt.xlabel('normalized force')\n",
    "plt.ylabel('counts')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper left')\n",
    "#plt.savefig('plots/two_smallest_pf_normalized_july-comparison.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contactsDict={file:sort_particles(dataDict[file]).groupby(['x','y','trialName']).size().reset_index(name='contacts') for file in (files)} # dataframe of particle coordinates, with contact number and trial labeled\n",
    "for file in tqdm_notebook(files):\n",
    "    contactsDict[file].set_index(['trialName','x','y'], inplace=True)\n",
    "    weakPins=dataDict[file][(dataDict[file].bondType>=3)&(dataDict[file].normalizedForce<=5e3)][['x1','y1','x2','y2','normalizedForce']].copy()\n",
    "    weakPins['contactsParticle']=weakPins.apply(lambda x: contactsDict[file].loc[x.name[0],x['x2'],x['y2']]['contacts'], axis=1)\n",
    "    weakPins['contactsPin']=weakPins.apply(lambda x: contactsDict[file].loc[x.name[0],x['x1'],x['y1']]['contacts'], axis=1)\n",
    "    allPins=dataDict[file][(dataDict[file].bondType>=3)][['x1','y1','x2','y2','normalizedForce']].copy()\n",
    "    allPins['contactsParticle']=allPins.apply(lambda x: contactsDict[file].loc[x.name[0],x['x2'],x['y2']]['contacts'], axis=1)\n",
    "    allPins['contactsPin']=allPins.apply(lambda x: contactsDict[file].loc[x.name[0],x['x1'],x['y1']]['contacts'], axis=1)\n",
    "    fig, (axAll, axWeak) = plt.subplots(1,2, figsize=(12,4))\n",
    "    for ax, data in [(axAll, allPins), (axWeak, weakPins)]:\n",
    "        ax.hist(data['contactsParticle'],bins=np.linspace(2.5,7.5,6), density=True, label='particle contacts')\n",
    "        ax.hist(data['contactsPin'],bins=np.linspace(0.5,2.5,3), density=True, label = 'pin contacts')\n",
    "        ax.set_ylim(0,0.9)\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.grid(linestyle='dashed')\n",
    "        ax.set_xlabel('number of contacts')\n",
    "    axWeak.set_title('weak contacts, normalized force < 0.005')\n",
    "    axAll.set_title('all contacts')\n",
    "    axWeak.legend()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    plt.suptitle('normalized distribution of particle/pin contacts, {}'.format(file), fontsize=14, x=0.52)\n",
    "    #plt.savefig('plots/particlePinContactNumbers/particle_pin_contact_distribution-{}.png'.format(file),dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDF = pd.DataFrame()\n",
    "weakDF = pd.DataFrame()\n",
    "for file in tqdm_notebook(files):\n",
    "    contactsDict={file:sort_particles(dataDict[file]).groupby(['x','y','trialName']).size().reset_index(name='contacts') for file in (files)} # dataframe of particle coordinates, with contact number and trial labeled\n",
    "    contactsDict[file].set_index(['trialName','x','y'], inplace=True)\n",
    "    weakPins=dataDict[file][(dataDict[file].bondType>=3)&(dataDict[file].normalizedForce<=5e-3)][['x1','y1','x2','y2','normalizedForce']].copy()\n",
    "    weakPins['contactsParticle']=weakPins.apply(lambda x: contactsDict[file].loc[x.name[0],x['x2'],x['y2']]['contacts'], axis=1)\n",
    "    weakPins['contactsPin']=weakPins.apply(lambda x: contactsDict[file].loc[x.name[0],x['x1'],x['y1']]['contacts'], axis=1)\n",
    "    allPins=dataDict[file][(dataDict[file].bondType>=3)][['x1','y1','x2','y2','normalizedForce']].copy()\n",
    "    allPins['contactsParticle']=allPins.apply(lambda x: contactsDict[file].loc[x.name[0],x['x2'],x['y2']]['contacts'], axis=1)\n",
    "    allPins['contactsPin']=allPins.apply(lambda x: contactsDict[file].loc[x.name[0],x['x1'],x['y1']]['contacts'], axis=1)\n",
    "    allDF = pd.concat([allDF,allPins[['contactsParticle', 'contactsPin']]])\n",
    "    weakDF = pd.concat([weakDF,weakPins[['contactsParticle','contactsPin']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axAll, axWeak) = plt.subplots(1,2, figsize=(12,4))\n",
    "for ax, data in [(axAll, allDF), (axWeak, weakDF)]:\n",
    "    ax.hist(data['contactsParticle'],bins=np.linspace(2.5,7.5,6), density=True, label='particle contacts')\n",
    "    ax.hist(data['contactsPin'],bins=np.linspace(0.5,2.5,3), density=True, label = 'pin contacts')\n",
    "    ax.set_ylim(0,0.9)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(linestyle='dashed')\n",
    "    ax.set_xlabel('number of contacts')\n",
    "axWeak.set_title('weak contacts, normalized force < 0.005')\n",
    "axAll.set_title('all contacts')\n",
    "axWeak.legend()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "plt.suptitle('normalized distribution of particle/pin contacts, aggregated', fontsize=14, x=0.52)\n",
    "#plt.savefig('plots/particlePinContactNumbers/particle_pin_contact_distribution-aggregate.png',dpi=300)\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analysis on Single Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Topology Plotting Function\n",
    "The plot_topography function takes a DataFrame and name of a trial and plots it. This primarily consists of preparing BondInfo data for plotting, which entails sorting particles by type, accounting for bonds which wrap at the boundary, and if the data is of a triangular lattice, applying a transformation before and after the other steps to make it compatible with our square lattice process.\n",
    "\n",
    "This is necessary as our triangle lattice data is provided with rhombus rather than square boundaries, so we need to pivot the data $30^\\circ$ to the left with respect to the x-axis to transform it into a square, account for wrapping, then pivot it back.\n",
    "\n",
    "Note that pins are drawn disproportionately large for reference, and since we're only given the small particle radius, the large radius is found by scaling the small radius up by a factor of 1.4. If this relation varies, we will need more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topography('non0_08-02',80, show=True, particles=True, pins=False, savefig='plots/presentation/jammed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import collections\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# What's really important: picking a fun color scheme\n",
    "#colormap = matplotlib.cycler('color', ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', 'r', '#FFFFFF', '#FFFFFF']) # classic\n",
    "colormap = matplotlib.cycler('color',['#FAD55C','#68C182','#267DB3','#ED6647','#8561C8','#000000', '#fef9e6', '#c0dff2']) # blue-yellow\n",
    "plt.rc('axes',prop_cycle=colormap)\n",
    "\n",
    "def plot_topography(file, trialName, savefig=False, title='Default', particles=True, pins=True, bonds=True, show=True, cutoff=None, sheared=True):\n",
    "    # PARAMETERS:\n",
    "    # file (str) - abbreviated file name, as specified list of files in first cell\n",
    "    # trialName (str or int) - full trial name, e.g. 'ran-90440-266-36-0.02781-.16667', or integer index of a certain trial\n",
    "    # savefig (bool/str) - If True, saves to plots folder. If string, saves to specified path.\n",
    "    # title (str) - name for figure, if not default.\n",
    "    # particles (bool) - whether or not to show particles.\n",
    "    # pins (str) - if \"all\", attempts to extrapolate the entire pin lattice from existing pins. If False, pins are not shown. Otherwise, plots only contacted pins.\n",
    "    # sheared (bool) - only significant for triangular lattices: if False, plots the data as a square instead ofa rhombus.\n",
    "    # show (bool) - whether or not display the figure in output.\n",
    "    # cutoff (float) - value at which to filter out forces. If positive, only show forces greater than the magnitude, and if negative, only show forces less than the magnitude.\n",
    "\n",
    "    if isinstance(trialName,int):\n",
    "        trialName=dataDict[file].index.get_level_values('trialName').unique()[trialName]\n",
    "\n",
    "    bondInfo=dataDict[file].loc[trialName].copy() # get the data\n",
    "    #find the mean force before filtering out bonds weaker than the cutoff\n",
    "    meanForce=bondInfo.force.abs().mean()\n",
    "    if cutoff:\n",
    "        if cutoff>0:\n",
    "            bondInfo=bondInfo[bondInfo.normalizedForce>=np.abs(cutoff)]\n",
    "        elif cutoff<0:\n",
    "            bondInfo=bondInfo[bondInfo.normalizedForce<=np.abs(cutoff)]\n",
    "\n",
    "    bondInfo['bondColor']=bondInfo['bondType'].transform(lambda x: 'C'+str(x)) # takes bondtypes and transforms them into color codes\n",
    "\n",
    "    headerNames=['geometry','seed','num','pins','small_radius','pin_separation']\n",
    "    trialInfo={}\n",
    "    for key, label in zip(headerNames,trialName.split('-')):\n",
    "        trialInfo[key]=label\n",
    "\n",
    "    # MAKING COORDINATE DATA USEFUL\n",
    "    all_particles = sort_particles(bondInfo)[['x','y','particleType']].drop_duplicates() # get dataframe of particle coordinates sorted by bond type\n",
    "    all_particles[['x','y']]=all_particles[['x','y']]*1e-6\n",
    "    bondInfo[['x1','y1','x2','y2','normalizedForce']]=bondInfo[['x1','y1','x2','y2','normalizedForce']]*1e-6\n",
    "    if trialInfo['geometry']=='tri': # extra steps for triangular lattice of pins\n",
    "        # Un-shear the data, so that we can apply the same wrapping algorithm as in the square\n",
    "        # unshearing matrix: [[1, -tan(30)],[0,1+tan(30)*tan(15)]] (in degrees)\n",
    "        for x,y in [('x1','y1'),('x2','y2')]: # apply transformation to both sets of coordinate columns\n",
    "            bondInfo[x]=bondInfo[x]-bondInfo[y]*np.tan(np.pi/6)\n",
    "            bondInfo[y]=bondInfo[y]*(1+np.tan(np.pi/6)*np.tan(np.pi/12))\n",
    "\n",
    "        if sheared==False: # normally we don't transform the particles at all, but if we want an unsheared plot we need to unshear the particles as well\n",
    "            all_particles['x']-=all_particles['y']*np.tan(np.pi/6)\n",
    "            all_particles['y']+=all_particles['y']*np.tan(np.pi/6)*np.tan(np.pi/12)\n",
    "\n",
    "    # Find wrapping contacts - bonds that are too long not to be wrapping\n",
    "    # (this is not memory efficient, but individual trials are small enough that we don't care)\n",
    "    wrapsX=bondInfo[np.abs(bondInfo.x1-bondInfo.x2)>=0.5].copy() # 0.5 is kind of arbitrary here, all that matters is distance > (R_A+R_B)\n",
    "    wrapsY=bondInfo[np.abs(bondInfo.y1-bondInfo.y2)>=0.5].copy()\n",
    "    internalBonds=bondInfo.drop(pd.concat([wrapsX,wrapsY]).index.values) # dataframe of non-wrapped contacts\n",
    "\n",
    "    # Wrapping algorithm: depending on which axis we're wrapping around, signs will flip - this value gives 1 or -1 for each wrapped contact according to sign convention\n",
    "    wrapsY['yAdjust']=round(wrapsY['y1']-wrapsY['y2'])\n",
    "    wrapsX['xAdjust']=round(wrapsX['x1']-wrapsX['x2'])\n",
    "\n",
    "    # Additional case for when bonds wrap through three regions via corner\n",
    "    wrapsCorner=pd.merge(wrapsX,wrapsY) # dataframe of contacts which wrap on X and Y, with both xAdjust and yAdjust included (corner cases have two sign conventions)\n",
    "    cornerIndex=wrapsX.index.intersection(wrapsY.index) # TODO: probably a more optimal way to do this, since we're effectively checking for duplicates twice on this line and the previous.\n",
    "\n",
    "    # Separate corner cases from edge cases (they'll plot differently)\n",
    "    wrapsX=wrapsX.drop(cornerIndex)\n",
    "    wrapsY=wrapsY.drop(cornerIndex)\n",
    "\n",
    "    # Reformat data for LineCollection plotting - TODO: optimize\n",
    "    # Internal lines\n",
    "    sourcePoints=np.array([internalBonds['x1'],internalBonds['y1']]).T.reshape(-1,1,2)\n",
    "    targetPoints=np.array([internalBonds['x2'],internalBonds['y2']]).T.reshape(-1,1,2)\n",
    "    internalsegments=np.concatenate([sourcePoints,targetPoints],axis=1)\n",
    "\n",
    "    # X-wrapped lines\n",
    "    # don't love the repetitive .values calls here, but in 1- or 0- length cases this preserves the shape of the array\n",
    "    sourcePoints=np.array([wrapsX['x1'].values-wrapsX['xAdjust'].values,wrapsX['y1'].values])\n",
    "    targetPoints=np.array([wrapsX['x2'].values,wrapsX['y2'].values])\n",
    "    sourcePoints=np.concatenate([sourcePoints,np.array([wrapsX['x1'].values,wrapsX['y1'].values])]).T.reshape(-1,1,2)\n",
    "    targetPoints=np.concatenate([targetPoints,np.array([wrapsX['x2'].values+wrapsX['xAdjust'].values,wrapsX['y2'].values])]).T.reshape(-1,1,2)\n",
    "    xsegments=np.concatenate([sourcePoints,targetPoints],axis=1)\n",
    "\n",
    "    # Y-wrapped lines\n",
    "    sourcePoints=np.array([wrapsY['x1'].values,wrapsY['y1'].values-wrapsY['yAdjust'].values])\n",
    "    targetPoints=np.array([wrapsY['x2'].values,wrapsY['y2'].values])\n",
    "    sourcePoints=np.concatenate([sourcePoints,np.array([wrapsY['x1'].values,wrapsY['y1'].values])]).T.reshape(-1,1,2)\n",
    "    targetPoints=np.concatenate([targetPoints,np.array([wrapsY['x2'].values,wrapsY['y2'].values+wrapsY['yAdjust'].values])]).T.reshape(-1,1,2)\n",
    "    ysegments=np.concatenate([sourcePoints,targetPoints],axis=1)\n",
    "\n",
    "    # Corner-wrapped lines\n",
    "    sourcePoints=np.array([]).reshape(-1,1,2)\n",
    "    targetPoints=np.array([]).reshape(-1,1,2)\n",
    "    for i in [0,1]: # 2x2 loop (one run for each corner)\n",
    "        for j in [0,1]:\n",
    "            sourcePoints=np.concatenate([sourcePoints,np.array([wrapsCorner['x1'].values+wrapsCorner['xAdjust'].values*(i-1),\n",
    "                                                                wrapsCorner['y1'].values+wrapsCorner['yAdjust'].values*(j-1)]).T.reshape(-1,1,2)])\n",
    "            targetPoints=np.concatenate([targetPoints,np.array([wrapsCorner['x2'].values+wrapsCorner['xAdjust'].values*(i),\n",
    "                                                                wrapsCorner['y2'].values+wrapsCorner['yAdjust'].values*(j)]).T.reshape(-1,1,2)])\n",
    "    cornersegments=np.concatenate([sourcePoints,targetPoints],axis=1)\n",
    "\n",
    "    if trialInfo['geometry']=='tri' and sheared==True:\n",
    "        # re-shear the data\n",
    "        # shearing matrix is [[1,sin(30)],[0,1-sin(30)*tan(15)]] (in degrees)\n",
    "        for segmentgroup in [internalsegments,xsegments,ysegments,cornersegments]: # bad form to iterate like this, but like above the arrays are small enough to get away with it\n",
    "            for segment in segmentgroup:\n",
    "                for point in segment:\n",
    "                    point[0]+=point[1]*np.sin(np.pi/6)\n",
    "                    point[1]-=point[1]*np.sin(np.pi/6)*np.tan(np.pi/12)\n",
    "\n",
    "    # Initialize patch collections\n",
    "    # lines are plotted with thickness normalizedForce+1 to scale with bond strength while all being visible\n",
    "    internalLines=collections.LineCollection(internalsegments,linewidths=internalBonds['normalizedForce']+1,color=internalBonds['bondColor'])\n",
    "    xLines=collections.LineCollection(xsegments,linewidths=wrapsX['normalizedForce'].repeat(2)+1,color=wrapsX['bondColor'].repeat(2))\n",
    "    yLines=collections.LineCollection(ysegments,linewidths=wrapsY['normalizedForce'].repeat(2),color=wrapsY['bondColor'].repeat(2))\n",
    "    cornerLines=collections.LineCollection(cornersegments,linewidths=wrapsCorner['normalizedForce'].repeat(4),color=wrapsCorner['bondColor'].repeat(4))\n",
    "\n",
    "    particleList=[]\n",
    "    if particles:\n",
    "        particleList+=[plt.Circle((x,y), radius=float(trialInfo['small_radius'])*1.4,ec='C5',fc='C7') for x,y in all_particles.loc[all_particles.particleType==2,['x','y']].values]\n",
    "        particleList+=[plt.Circle((x,y), radius=float(trialInfo['small_radius']),ec='C5',fc='C6') for x,y in all_particles.loc[all_particles.particleType==1,['x','y']].values]\n",
    "    if pins=='all' and trialInfo['geometry']!='ran':\n",
    "        allPinsInFile=sort_particles(dataDict[file],types=[0])\n",
    "        pinMin=allPinsInFile.min()*1e-6\n",
    "        pinMax=allPinsInFile.max()*1e-6\n",
    "        if trialInfo['geometry']=='tri':\n",
    "            pinMin['x']-=pinMin['y']*np.tan(np.pi/6)\n",
    "            pinMax['x']-=pinMax['y']*np.tan(np.pi/6)\n",
    "        pinGrid=np.meshgrid(np.linspace(pinMin['x'],pinMax['x'],int(np.sqrt(int(trialInfo['pins'])))),np.linspace(pinMin['x'],pinMax['x'],int(np.sqrt(int(trialInfo['pins'])))))\n",
    "        if trialInfo['geometry']=='tri' and sheared==True:\n",
    "            pinGrid[0]+=pinGrid[1]*np.sin(np.pi/6)\n",
    "            pinGrid[1]-=pinGrid[1]*np.sin(np.pi/6)*np.tan(np.pi/12)\n",
    "        particleList+=[plt.Circle((x,y), radius=0.005, color='k',fill=True) for x,y in np.vstack([pinGrid[0].flatten(),pinGrid[1].flatten()]).T]\n",
    "    elif pins==True:\n",
    "        particleList+=[plt.Circle((x,y), radius=0.005, color='k',fill=True) for x,y in all_particles.loc[all_particles.particleType==0,['x','y']].values]\n",
    "\n",
    "    ParticleCollection=collections.PatchCollection(particleList, match_original=True)\n",
    "\n",
    "    plt.figure(figsize=(8,8)) # large, square plot\n",
    "    a = plt.subplot() # initialize axes\n",
    "\n",
    "    # Plot lines\n",
    "    a.set(aspect='equal') # plot axes to scale\n",
    "    if bonds:\n",
    "        a.add_collection(internalLines)\n",
    "        a.add_collection(xLines)\n",
    "        a.add_collection(yLines)\n",
    "        a.add_collection(cornerLines)\n",
    "\n",
    "    # Plot particles\n",
    "    a.add_collection(ParticleCollection)\n",
    "\n",
    "    # Cosmetics\n",
    "    if trialInfo['geometry']=='tri' and sheared==True: # for the rhombus, we need different axis limits\n",
    "        a.set_xlim(0,1.5)\n",
    "        a.set_ylim(-0.1,np.sqrt(3)/2+0.1)\n",
    "        legendLocation='lower right'\n",
    "    else:\n",
    "        a.set_xlim(0,1)\n",
    "        a.set_ylim(0,1)\n",
    "        legendLocation='upper right'\n",
    "    a.set_xlabel('x')\n",
    "    a.set_ylabel('y')\n",
    "    if title=='Default':\n",
    "        a.set_title('Topography of {} data, {} total, seed {}, mean force={mf:.4f}'.format(file, *[trialInfo[x] for x in ['num','seed']], mf=meanForce))\n",
    "    else:\n",
    "        a.set_title(title)\n",
    "\n",
    "    # Particle legend\n",
    "    # can't directly label collections, so we designate shapes just for the legends\n",
    "    a.scatter([],[],facecolors='C7',edgecolors='C5', s=150, label='large grains')\n",
    "    a.scatter([],[],facecolors='C6',edgecolors='C5', s=100, label = 'small grains')\n",
    "    a.scatter([],[],c='k', s=30,label='pins')\n",
    "\n",
    "    particleLegend1 = a.legend(loc=legendLocation,title='Particle Types',framealpha=0.95)\n",
    "    a.add_artist(particleLegend1) # matplotlib also overwrites with the latest legend call by default, so we specify we want this one as-is\n",
    "\n",
    "    # Line legend\n",
    "    custom_lines1 = [Line2D([0], [0], color='C0', lw=2),\n",
    "                    Line2D([0], [0], color='C1', lw=2),\n",
    "                    Line2D([0], [0], color='C2', lw=2),\n",
    "                    Line2D([0], [0], color='C3', lw=2),\n",
    "                    Line2D([0], [0], color='C4', lw=2)]\n",
    "    lineLegend = a.legend(custom_lines1,['small-small','small-large','large-large','pin-small','pin-large'],loc='upper left',title='Contact Types',framealpha=0.95)\n",
    "\n",
    "    if savefig:\n",
    "        if isinstance(savefig,str):\n",
    "            if not os.path.exists('/'.join(savefig.split('/')[:-1])):\n",
    "                os.makedirs('/'.join(savefig.split('/')[:-1]))\n",
    "            if cutoff:\n",
    "                saveName = '{}-cutoff_{}.png'.format(savefig, np.abs(cutoff))\n",
    "            else:\n",
    "                saveName = '{}.png'.format(savefig)\n",
    "        else:\n",
    "            saveName = 'plots/{}.png'.format(trialName)\n",
    "            print('Directory not specified, saving to plots folder')\n",
    "        plt.savefig(saveName, dpi=300)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    try:\n",
    "        return saveName\n",
    "    except:\n",
    "        return trialName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    plot_topography(file, 0, pins='all', particles=False, sheared=False, savefig='plots/forAmy_allPins', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Animating Increasing Bond Strength Cutoff for a Trial/Identifying and Plotting Exceptionally Large Forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "\n",
    "for file in tqdm_notebook(['squ100_07-19']):\n",
    "    print('FILE:', filenames[file])\n",
    "    # Plotting exceptions\n",
    "    for trialName in tqdm_notebook(dataDict[file]['normalizedForce'].nlargest(10).index.get_level_values(0).unique()):\n",
    "        plot_topography(file, trialName, savefig='plots/exceptions', title='{} exception case, trial name {}'.format(file, trialName))\n",
    "\n",
    "    # Creating Animations\n",
    "    #for trialName in tqdm_notebook(dataDict[file]['normalizedForce'].nlargest(10).index.get_level_values(0).unique()):\n",
    "    #    image_list=[]\n",
    "    #    if not os.path.exists('Bond_Animations/'+trialName):\n",
    "    #        os.makedirs('Bond_Animations/'+trialName)\n",
    "    #    print('TRIAL:', trialName)\n",
    "    #    for cutoffForce in tqdm_notebook([-100.0,-1.0,-0.9,-0.8,-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1]):\n",
    "    #        saveName=plot_topography(file, trialName, savefig='Bond_Animations/'+trialName, cutoff=cutoffForce, title='{} trial name {}, force cutoff {}'.format(file, trialName, np.abs(cutoffForce)))\n",
    "    #        image_list.append(imageio.imread(saveName))\n",
    "    #    imageio.mimwrite('Bond_Animations/{}/animated.gif'.format(trialName), image_list, duration = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict['tri100_07-19']['normalizedForce'].nlargest(40).index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. (Deprecated). Plotting lines directly in MatPlotLib\n",
    "This code is fast, but plt.plot() can't account for varying line width; we end up using a LineCollection to do this in the next cell. Leaving this here for posterity."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8,8)) # large, square plot\n",
    "# Plot internal bonds\n",
    "plt.plot([internalBonds['x1'],internalBonds['x2']],[internalBonds['y1'],internalBonds['y2']],'C0')\n",
    "# Plot bonds which wrap around boundary\n",
    "plt.plot([wrapsY['x1'],wrapsY['x2']],[wrapsY['y1']-wrapsY['yAdjust'],wrapsY['y2']],'C3')\n",
    "plt.plot([wrapsY['x1'],wrapsY['x2']],[wrapsY['y1'],wrapsY['y2']+wrapsY['yAdjust']],'C3')\n",
    "plt.plot([wrapsX['x1']-wrapsX['xAdjust'],wrapsX['x2']],[wrapsX['y1'],wrapsX['y2']],'C1')\n",
    "plt.plot([wrapsX['x1'],wrapsX['x2']+wrapsX['xAdjust']],[wrapsX['y1'],wrapsX['y2']],'C1')\n",
    "# Change axis limits to size of simulation\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "#plt.savefig('plots/sampleNetwork.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. (Deprecated). Plot with particleinfo.txt\n",
    "A sample plotting of data from the last cell, with particles extrapolated from simulation_data/june_18/squ_BondInfo.txt, compared against a plot with particle types and locations explicitly given by a particleinfo.txt file. Note that with an additional file, we can pinpoint the location of rattlers and untouched pins."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from matplotlib import collections\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "internalLines=collections.LineCollection(internalsegments,linewidths=internalBonds['normalizedForce']+1,color=internalBonds['bondColor'])\n",
    "xLines=collections.LineCollection(xsegments,linewidths=wrapsX['normalizedForce'].repeat(2)+1,color=wrapsX['bondColor'].repeat(2))\n",
    "yLines=collections.LineCollection(ysegments,linewidths=wrapsY['normalizedForce'].repeat(2)+1,color=wrapsY['bondColor'].repeat(2))\n",
    "\n",
    "largeParticles=[plt.Circle((x,y), radius=float(trialInfo['small_radius'])*1.4, color='r',fill=False) for x,y in large_mobile[['x','y']].values]\n",
    "smallParticles=[plt.Circle((x,y), radius=float(trialInfo['small_radius']), color='r',fill=False) for x,y in small_mobile[['x','y']].values]\n",
    "pinList=[plt.Circle((x,y), radius=0.005, color='k',fill=True) for x,y in pins[['x','y']].values]\n",
    "bondParticleCollection=collections.PatchCollection(largeParticles+smallParticles+pinList, match_original=True)\n",
    "#plt.figure(figsize=(16,8)) # large, square plot\n",
    "fig, (ax1,ax2) = plt.subplots(1,2) # initialize axes\n",
    "\n",
    "# Plot lines\n",
    "ax1.set(aspect='equal')\n",
    "ax1.add_collection(internalLines)\n",
    "ax1.add_collection(xLines)\n",
    "ax1.add_collection(yLines)\n",
    "\n",
    "# Plot particles\n",
    "ax1.add_collection(bondParticleCollection)\n",
    "\n",
    "# Cosmetics\n",
    "ax1.set_xlim(0,1)\n",
    "ax1.set_ylim(0,1)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Particle types reverse-engineered from bond types')\n",
    "#Topology of 100 pin square lattice, 256 mobile, seed 80002\n",
    "# Particle legend\n",
    "# can't directly label collections, so we designate shapes julst for the legends\n",
    "ax1.scatter([],[],facecolors='none',edgecolors='r', s=250, label='large grains')\n",
    "ax1.scatter([],[],facecolors='none',edgecolors='r', s=100, label = 'small grains')\n",
    "ax1.scatter([],[],c='k', s=30,label='pins')\n",
    "\n",
    "particleLegend1 = ax1.legend(loc='upper right',title='Particle Types',framealpha=0.95)\n",
    "ax1.add_artist(particleLegend1)\n",
    "\n",
    "# Line legend\n",
    "custom_lines1 = [Line2D([0], [0], color='C0', lw=2),\n",
    "                Line2D([0], [0], color='C1', lw=2),\n",
    "                Line2D([0], [0], color='C2', lw=2),\n",
    "                Line2D([0], [0], color='C3', lw=2),\n",
    "                Line2D([0], [0], color='C4', lw=2)]\n",
    "lineLegend = ax1.legend(custom_lines1,['small-small','small-large','large-large','pin-small','pin-large'],loc='upper left',title='Bond Types',framealpha=0.95)\n",
    "\n",
    "\n",
    "# reading particleinfo file\n",
    "particleInfo=pd.read_csv('simulation_data/june_18/particleInfo.txt', sep=' ', header=None, names=['particle_number', 'x', 'y', 'radius', 'is_rattler', 'is_pin_or_rattler'],usecols=[1,2,3,4,5])\n",
    "\n",
    "# identify pin radius to distinguish from rattlers\n",
    "pin_radius=particleInfo['radius'].min()\n",
    "\n",
    "# Creating cosmetic columns\n",
    "particleInfo['color']='r'\n",
    "particleInfo['fill']=False\n",
    "particleInfo['alpha']=1\n",
    "particleInfo.loc[particleInfo.radius==pin_radius, ['color','fill','radius']] = 'k', True, 0.005\n",
    "particleInfo.loc[particleInfo.is_rattler==1, ['color','fill','alpha']] = 'silver', True, 0.6\n",
    "\n",
    "internalLines=collections.LineCollection(internalsegments,linewidths=internalBonds['normalizedForce']+1,color=internalBonds['bondColor'])\n",
    "xLines=collections.LineCollection(xsegments,linewidths=wrapsX['normalizedForce'].repeat(2)+1,color=wrapsX['bondColor'].repeat(2))\n",
    "yLines=collections.LineCollection(ysegments,linewidths=wrapsY['normalizedForce'].repeat(2)+1,color=wrapsY['bondColor'].repeat(2))\n",
    "\n",
    "particleList=[plt.Circle((x,y), radius=r, color=color,fill=fill,alpha=alpha) for x,y,r,color,fill,alpha in particleInfo[['x','y','radius','color','fill','alpha']].values]\n",
    "particles=collections.PatchCollection(particleList,match_original=True)\n",
    "\n",
    "# Cosmetics\n",
    "ax2.set_xlim(0,1)\n",
    "ax2.set_ylim(0,1)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Particle types given from particleinfo.txt')\n",
    "\n",
    "ax2.set(aspect='equal')\n",
    "ax2.add_collection(particles)\n",
    "ax2.add_collection(internalLines)\n",
    "ax2.add_collection(xLines)\n",
    "ax2.add_collection(yLines)\n",
    "\n",
    "# Particle legend\n",
    "# can't directly label collections, so we designate shapes just for the legends\n",
    "ax2.scatter([],[],facecolors='none',edgecolors='r', s=250, label='large grains')\n",
    "ax2.scatter([],[],facecolors='none',edgecolors='r', s=100, label = 'small grains')\n",
    "ax2.scatter([],[],facecolors='silver',edgecolors='silver',alpha=0.8, s=100, label = 'rattlers')\n",
    "ax2.scatter([],[],c='k', s=30,label='pins')\n",
    "particleLegend2 = ax2.legend(loc='upper right',title='Particle Types',framealpha=0.95)\n",
    "ax2.add_artist(particleLegend2)\n",
    "\n",
    "# Line legend\n",
    "custom_lines2 = [Line2D([0], [0], color='C0', lw=2),\n",
    "                Line2D([0], [0], color='C1', lw=2),\n",
    "                Line2D([0], [0], color='C2', lw=2),\n",
    "                Line2D([0], [0], color='C3', lw=2),\n",
    "                Line2D([0], [0], color='C4', lw=2)]\n",
    "lineLegend = ax2.legend(custom_lines2,['small-small','small-large','large-large','pin-small','pin-large'],loc='upper left',title='Bond Types',framealpha=0.95)\n",
    "#plt.savefig('plots/topology_sourcefile_comparison.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 (Deprecated). Plot contact force distribution as histogram for a single trial\n",
    "Plots normalized contact forces $\\frac{f}{\\langle f\\rangle}$ using seaborn's distplot() function. At this point, the y axis is not normalized; the kde parameter allows for normalization for a single data set, but to normalize the second plot collectively appears challenging."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig2 = plt.figure(figsize=(10,12))\n",
    "\n",
    "hist1=fig2.add_subplot(2,1,1)\n",
    "sns.distplot(bondInfo['normalizedForce'], hist=True, kde=False,\n",
    "             bins=40, color = 'darkblue',\n",
    "             hist_kws={'histtype':'step','linewidth':3,'alpha':0.95},\n",
    "             #kde_kws={'linewidth': 0}\n",
    "             )\n",
    "hist1.set_xlim(-0.15,3.8)\n",
    "hist1.set_title('force distribution of {} pin {} lattice, {} mobile, seed {}'.format(*[trialInfo[x] for x in ['pins','geometry','num','seed']]))\n",
    "hist1.set_xlabel('')\n",
    "#hist1.set_ylabel('probability density')\n",
    "\n",
    "hist2=fig2.add_subplot(2,1,2)\n",
    "bondInfo.groupby('bondType')['normalizedForce'].apply(lambda x: sns.distplot(x, hist=True, kde=False, bins=40, hist_kws={'histtype':'step','linewidth':3,'alpha':0.95}, label='bondType {}'.format(x.name)))\n",
    "hist2.set_title('distribution by bond type')\n",
    "hist2.set_xlabel('normalized contact forces')\n",
    "hist2.set_xlim(-0.15,3.8)\n",
    "hist2.legend()\n",
    "#plt.savefig('plots/june_18_force_distribution_histogram.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
